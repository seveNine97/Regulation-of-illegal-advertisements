# 代码示例
 1.OCR＋规则匹配
 ```# 依赖: pip install pillow pytesseract opencv-python requests rapidfuzz
import io, requests, re, unicodedata
from PIL import Image, ImageFilter, ImageOps
import pytesseract
from rapidfuzz import fuzz  # 模糊匹配（同音/插入字符检测）

# -- 图片下载与预处理 --
def download_image(url, timeout=5):
    resp = requests.get(url, timeout=timeout)
    return Image.open(io.BytesIO(resp.content)).convert('RGB')

def preprocess_for_ocr(img_pil):
    # 灰度、放大、锐化、二值化等常见预处理
    img = img_pil.convert('L')
    img = img.resize((int(img.width*1.5), int(img.height*1.5)), Image.BILINEAR)
    img = ImageOps.autocontrast(img)
    img = img.filter(ImageFilter.MedianFilter(size=3))
    return img

# -- OCR 提取文字（返回文本与 bbox 列表） --
def ocr_image(img_pil):
    # pytesseract 的 data 形式可得到 bbox
    data = pytesseract.image_to_data(img_pil, output_type=pytesseract.Output.DATAFRAME)
    text_pieces = []
    for _, row in data.dropna(subset=['text']).iterrows():
        txt = str(row['text']).strip()
        if txt:
            text_pieces.append(txt)
    full_text = " ".join(text_pieces)
    return full_text

# -- 规范化文本（去零宽，统一大小写，去特殊） --
def normalize_text(s):
    if s is None:
        return ""
    s = unicodedata.normalize('NFKC', s)
    s = s.replace('\u200b', '')  # 零宽空格
    s = s.strip().lower()
    # 基本字符映射（可以扩充）
    s = s.replace('＠','@').replace('＃','#')
    return s

# -- 规则匹配 (示例) --
KEYWORD_PATTERNS = [
    r"100%中奖", r"免费\s*i?h?o?n?e?", r"贷款.*无利息", r"zero\s*fee"
]
def rule_check(text):
    text = normalize_text(text)
    # 精确/正则匹配
    for p in KEYWORD_PATTERNS:
        if re.search(p, text, flags=re.I):
            return True, f"regex:{p}"
    # 模糊检测（处理插空格/同音/字符替换）
    for kw in ["free", "iPhone", "中奖", "返现"]:
        score = fuzz.partial_ratio(kw, text)
        if score > 85:  # 阈值可调
            return True, f"fuzzy:{kw}:{score}"
    return False, None

# -- end-to-end for an image
def process_image_url(url):
    img = download_image(url)
    img_pre = preprocess_for_ocr(img)
    ocr_text = ocr_image(img_pre)
    blocked, reason = rule_check(ocr_text)
    return {
        "ocr_text": ocr_text,
        "rule_block": blocked,
        "rule_reason": reason
    }

# Example usage:
# res = process_image_url("https://example.com/ad_creative.jpg")
# print(res)```


2.
# 依赖: pip install sentence-transformers transformers torchvision imbalanced-learn lightgbm
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import lightgbm as lgb
import joblib

# 假设 df 包含: ad_text, ocr_text, image_path (或 image_url), numeric cols ['ctr','impr_last_1h'], label
df = pd.read_parquet("data/ads_features.parquet")  # 示例

# --- 1) 文本 embedding (sentence-transformers) ---
from sentence_transformers import SentenceTransformer
text_model = SentenceTransformer('all-MiniLM-L6-v2')  # 句子嵌入，轻量可用CPU
df['text_combined'] = (df['ad_text'].fillna('') + ' ' + df['ocr_text'].fillna('')).str.strip()
text_embs = text_model.encode(df['text_combined'].tolist(), show_progress_bar=True, batch_size=64)
# text_embs.shape = (n_samples, emb_dim)

# --- 2) 图片 embedding (CLIP via sentence-transformers)
# Note: can use 'clip-ViT-B-32' model from sentence-transformers
try:
    from sentence_transformers import SentenceTransformer as ST2
    img_model = ST2('clip-ViT-B-32')
    # load images in list of PIL.Image
    def load_image(path_or_url):
        from PIL import Image
        import requests, io
        if path_or_url.startswith('http'):
            r = requests.get(path_or_url, timeout=5)
            return Image.open(io.BytesIO(r.content)).convert('RGB')
        return Image.open(path_or_url).convert('RGB')
    imgs = [load_image(p) if pd.notna(p) else None for p in df['image_path'].fillna('')]
    # replace None with a small blank image to keep alignment
    from PIL import Image
    blank = Image.new('RGB', (224,224), color=(255,255,255))
    imgs = [img if img is not None else blank for img in imgs]
    img_embs = img_model.encode(imgs, batch_size=32, show_progress_bar=True)
except Exception as e:
    print("Image model unavailable, fallback to zeros", e)
    img_embs = np.zeros((len(df), 512))

# --- 3) numeric features
num_cols = ['ctr', 'impr_last_1h', 'advertiser_violation_rate']
X_num = df[num_cols].fillna(0).values

# --- 4) 拼接特征
X_full = np.hstack([text_embs, img_embs, X_num])
y = df['label'].astype(int).values

# --- 5) 划分（时间切分优先）: 这里示例随机分割，生产请按时间切分
X_train, X_val, y_train, y_val = train_test_split(X_full, y, test_size=0.2, random_state=42, stratify=y)

# --- 6) 若不平衡: SMOTE (先降维再 SMOTE 如果维度太大)
# 先 PCA/SVD 降到 64 维再 SMOTE
svd = TruncatedSVD(n_components=64, random_state=42)
X_train_svd = svd.fit_transform(X_train)
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train_svd, y_train)

# 需把 resampled 数据映射回 train 特征空间时的处理：
# 方案A: 在 SVD 后训练一个轻量级模型 (e.g., Logistic) 使用 X_train_res
# 方案B: 用 resampled 的低维数据训练模型，并保存 svd->model 流程

# 示例：在低维上训练 LightGBM（快速示例）
clf = lgb.LGBMClassifier(n_estimators=200)
clf.fit(X_train_res, y_train_res, eval_set=[(svd.transform(X_val), y_val)], early_stopping_rounds=20)

# 保存模型与预处理器
joblib.dump({'svd': svd, 'clf': clf, 'text_model_name': 'all-MiniLM-L6-v2'}, "models/ad_model.pkl")

3.from sklearn.metrics import precision_recall_curve, confusion_matrix
import numpy as np

# 假设 clf 已训练，X_val_prob 为 clf.predict_proba(X_val)[:,1], y_val 为真实标签
probs = clf.predict_proba(svd.transform(X_val))[:,1]  # 或直接 clf.predict_proba(X_val_emb)[...,1]
prec, rec, thr = precision_recall_curve(y_val, probs)

# 业务成本设置（需要与你的产品/运营沟通）
C_FP = 1.0   # 人工复核成本/被误判带来的平均损失（单位化）
C_FN = 50.0  # 一个漏判违规造成的平均业务损失（例如投诉、合规罚款更高）

# 给每个阈值计算预期成本（注意 precision,rec arrays length > thresholds len）
# thresholds correspond to points between precision/rec arrays; we compute predicted labels for threshold t and compute cost
best_threshold = 0.5
best_cost = float('inf')
for t in np.linspace(0.01, 0.99, 99):
    y_pred = (probs >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()
    cost = C_FP * fp + C_FN * fn
    if cost < best_cost:
        best_cost = cost
        best_threshold = t

print("Best threshold by cost:", best_threshold, "cost:", best_cost)

# 另一种做法：选择使 recall >= target 而 precision 尽可能高
target_recall = 0.8
candidates = [(p,r,t) for p,r,t in zip(prec,rec, list(thr)+[1.0]) if r>=target_recall]
if candidates:
    best = max(candidates, key=lambda x: x[0])  # choose max precision among recall >= target
    print("Threshold for recall>=%.2f is %.3f (precision=%.3f)" % (target_recall, best[2], best[0]))


4.
import shap, numpy as np, pandas as pd
# 假设训练时使用: svd (or original features), clf (LightGBM), X_train/X_val as dense arrays

# 1) 选取关注样本（例如验证集中模型错判的样本）
probs_val = clf.predict_proba(svd.transform(X_val))[:,1]
y_pred_val = (probs_val >= best_threshold).astype(int)
mask_fn = (y_val == 1) & (y_pred_val == 0)  # False negatives
X_fn = svd.transform(X_val)[mask_fn]

# 2) 计算 SHAP（对树模型效率高）
explainer = shap.TreeExplainer(clf)
# 注意：如果模型是概率输出，TreeExplainer 返回对 log-odds 或 output 的 shap 值，使用解释方法时要看文档
shap_vals = explainer.shap_values(X_fn)  # 对二分类，可能返回 [class0_vals, class1_vals]

# 3) 全局总结（依赖于 shap_values 结构）
# 如果 shap_values 是 list (二分类), 选择 class 1
if isinstance(shap_vals, list):
    shap_vals_cls1 = shap_vals[1]
else:
    shap_vals_cls1 = shap_vals

# 求特征的平均绝对值重要性
mean_abs_shap = np.abs(shap_vals_cls1).mean(axis=0)
top_idx = np.argsort(mean_abs_shap)[-20:][::-1]
print("Top contributing features (FN samples):")
for idx in top_idx:
    print(f"feature_{idx}, mean_abs_shap={mean_abs_shap[idx]:.4f}")

# 4) 可视化（在 notebook 中）
shap.summary_plot(shap_vals_cls1, X_fn, feature_names=feature_names)  # 需要 feature_names 列表

# 5) 局部解释：查看某个样本
sample_idx = 0
shap.force_plot(explainer.expected_value[1], shap_vals_cls1[sample_idx], X_fn[sample_idx], feature_names=feature_names)
